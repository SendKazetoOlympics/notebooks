{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W319 22:11:48.829486913 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n",
      "  Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n",
      "    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: XPU\n",
      "  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n",
      "       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W319 22:11:50.372075803 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n",
      "  Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n",
      "    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: XPU\n",
      "  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n",
      "       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from ultralytics import YOLO, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'data/models/yolo11x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109M/109M [00:03<00:00, 34.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.91 üöÄ Python-3.11.11 torch-2.6.0+xpu CPU (AMD Ryzen 5 7600X 6-Core Processor)\n",
      "WARNING ‚ö†Ô∏è imgsz=[1080, 1920] must be multiple of max stride 32, updating to [1088, 1920]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11x summary (fused): 190 layers, 56,919,424 parameters, 0 gradients, 194.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'data/models/yolo11x.pt' with input shape (1, 3, 1088, 1920) BCHW and output shape(s) (1, 84, 42840) (109.3 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['openvino>=2024.0.0,!=2025.0.0'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openvino!=2025.0.0,>=2024.0.0 (from versions: 2025.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openvino!=2025.0.0,>=2024.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.0.0-17942-1f68be9f594-releases/2025/0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openvino!=2025.0.0,>=2024.0.0 (from versions: 2025.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openvino!=2025.0.0,>=2024.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ‚úÖ 18.9s, saved as 'data/models/yolo11x_openvino_model/' (218.2 MB)\n",
      "\n",
      "Export complete (24.3s)\n",
      "Results saved to \u001b[1m/home/kazewong/data/jupyter/models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=data/models/yolo11x_openvino_model imgsz=1088,1920  \n",
      "Validate:        yolo val task=detect model=data/models/yolo11x_openvino_model imgsz=1088,1920 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml  WARNING ‚ö†Ô∏è non-PyTorch val requires square images, 'imgsz=[1088, 1920]' will not work. Use export 'imgsz=1920' if val is required.\n",
      "Visualize:       https://netron.app\n",
      "WARNING ‚ö†Ô∏è Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "WARNING ‚ö†Ô∏è Environment does not support cv2.imshow() or PIL Image.show()\n",
      "The DISPLAY environment variable isn't set.\n",
      "Loading data/models/yolo11x_openvino_model for OpenVINO inference...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['openvino>=2024.0.0,!=2025.0.0'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openvino!=2025.0.0,>=2024.0.0 (from versions: 2025.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openvino!=2025.0.0,>=2024.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/2 failed: Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "Retry 2/2 failed: Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ùå Command 'pip install --no-cache-dir \"openvino>=2024.0.0,!=2025.0.0\" ' returned non-zero exit status 1.\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openvino!=2025.0.0,>=2024.0.0 (from versions: 2025.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openvino!=2025.0.0,>=2024.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING ‚ö†Ô∏è imgsz=[1080, 1920] must be multiple of max stride 32, updating to [1088, 1920]\n",
      "\n",
      "WARNING ‚ö†Ô∏è inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 clock, 703.5ms\n",
      "video 1/1 (frame 2/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 chair, 1 clock, 693.5ms\n",
      "video 1/1 (frame 3/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 chair, 1 clock, 695.4ms\n",
      "video 1/1 (frame 4/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 bicycle, 1 clock, 685.5ms\n",
      "video 1/1 (frame 5/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 bicycle, 1 clock, 79.6ms\n",
      "video 1/1 (frame 6/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 bicycle, 1 chair, 1 clock, 41.1ms\n",
      "video 1/1 (frame 7/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 bicycle, 1 clock, 39.9ms\n",
      "video 1/1 (frame 8/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 bicycle, 1 clock, 37.5ms\n",
      "video 1/1 (frame 9/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 bicycle, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 10/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 bicycle, 1 clock, 37.0ms\n",
      "video 1/1 (frame 11/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 bicycle, 1 chair, 1 clock, 36.9ms\n",
      "video 1/1 (frame 12/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 36.5ms\n",
      "video 1/1 (frame 13/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.3ms\n",
      "video 1/1 (frame 14/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 37.0ms\n",
      "video 1/1 (frame 15/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 37.3ms\n",
      "video 1/1 (frame 16/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.9ms\n",
      "video 1/1 (frame 17/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.7ms\n",
      "video 1/1 (frame 18/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.5ms\n",
      "video 1/1 (frame 19/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.8ms\n",
      "video 1/1 (frame 20/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.6ms\n",
      "video 1/1 (frame 21/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 38.5ms\n",
      "video 1/1 (frame 22/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.8ms\n",
      "video 1/1 (frame 23/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 38.5ms\n",
      "video 1/1 (frame 24/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 25/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 26/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 37.0ms\n",
      "video 1/1 (frame 27/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.4ms\n",
      "video 1/1 (frame 28/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 train, 1 chair, 1 clock, 36.7ms\n",
      "video 1/1 (frame 29/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.3ms\n",
      "video 1/1 (frame 30/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 31/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.6ms\n",
      "video 1/1 (frame 32/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.4ms\n",
      "video 1/1 (frame 33/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 34/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 36.4ms\n",
      "video 1/1 (frame 35/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.3ms\n",
      "video 1/1 (frame 36/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 36.6ms\n",
      "video 1/1 (frame 37/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.6ms\n",
      "video 1/1 (frame 38/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.8ms\n",
      "video 1/1 (frame 39/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 clock, 36.9ms\n",
      "video 1/1 (frame 40/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 clock, 36.9ms\n",
      "video 1/1 (frame 41/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 train, 1 clock, 36.9ms\n",
      "video 1/1 (frame 42/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 6 persons, 1 chair, 1 clock, 37.1ms\n",
      "video 1/1 (frame 43/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 36.1ms\n",
      "video 1/1 (frame 44/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.0ms\n",
      "video 1/1 (frame 45/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.3ms\n",
      "video 1/1 (frame 46/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 (no detections), 37.2ms\n",
      "video 1/1 (frame 47/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.6ms\n",
      "video 1/1 (frame 48/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.8ms\n",
      "video 1/1 (frame 49/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 36.9ms\n",
      "video 1/1 (frame 50/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.2ms\n",
      "video 1/1 (frame 51/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.5ms\n",
      "video 1/1 (frame 52/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.0ms\n",
      "video 1/1 (frame 53/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.6ms\n",
      "video 1/1 (frame 54/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.6ms\n",
      "video 1/1 (frame 55/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 37.0ms\n",
      "video 1/1 (frame 56/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 36.3ms\n",
      "video 1/1 (frame 57/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 2 clocks, 36.9ms\n",
      "video 1/1 (frame 58/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.7ms\n",
      "video 1/1 (frame 59/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.4ms\n",
      "video 1/1 (frame 60/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.8ms\n",
      "video 1/1 (frame 61/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 clock, 36.7ms\n",
      "video 1/1 (frame 62/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 clock, 36.6ms\n",
      "video 1/1 (frame 63/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 5 persons, 1 chair, 1 clock, 36.2ms\n",
      "video 1/1 (frame 64/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 7 persons, 1 chair, 1 clock, 36.8ms\n",
      "video 1/1 (frame 65/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 6 persons, 1 chair, 1 clock, 36.5ms\n",
      "video 1/1 (frame 66/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 6 persons, 1 chair, 1 clock, 36.5ms\n",
      "video 1/1 (frame 67/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 36.6ms\n",
      "video 1/1 (frame 68/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.9ms\n",
      "video 1/1 (frame 69/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 35.5ms\n",
      "video 1/1 (frame 70/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.2ms\n",
      "video 1/1 (frame 71/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 36.0ms\n",
      "video 1/1 (frame 72/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.1ms\n",
      "video 1/1 (frame 73/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.5ms\n",
      "video 1/1 (frame 74/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.6ms\n",
      "video 1/1 (frame 75/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.3ms\n",
      "video 1/1 (frame 76/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 clock, 37.1ms\n",
      "video 1/1 (frame 77/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 36.2ms\n",
      "video 1/1 (frame 78/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 37.2ms\n",
      "video 1/1 (frame 79/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 36.5ms\n",
      "video 1/1 (frame 80/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 36.5ms\n",
      "video 1/1 (frame 81/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 clock, 36.6ms\n",
      "video 1/1 (frame 82/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 37.0ms\n",
      "video 1/1 (frame 83/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 bicycle, 1 clock, 36.7ms\n",
      "video 1/1 (frame 84/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 bicycle, 1 clock, 37.0ms\n",
      "video 1/1 (frame 85/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.1ms\n",
      "video 1/1 (frame 86/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 clock, 37.1ms\n",
      "video 1/1 (frame 87/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 clock, 36.8ms\n",
      "video 1/1 (frame 88/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.2ms\n",
      "video 1/1 (frame 89/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.3ms\n",
      "video 1/1 (frame 90/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.7ms\n",
      "video 1/1 (frame 91/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.4ms\n",
      "video 1/1 (frame 92/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.3ms\n",
      "video 1/1 (frame 93/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 (no detections), 36.6ms\n",
      "video 1/1 (frame 94/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 37.3ms\n",
      "video 1/1 (frame 95/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.2ms\n",
      "video 1/1 (frame 96/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.1ms\n",
      "video 1/1 (frame 97/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.3ms\n",
      "video 1/1 (frame 98/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 (no detections), 37.1ms\n",
      "video 1/1 (frame 99/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.8ms\n",
      "video 1/1 (frame 100/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.3ms\n",
      "video 1/1 (frame 101/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.1ms\n",
      "video 1/1 (frame 102/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.3ms\n",
      "video 1/1 (frame 103/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.3ms\n",
      "video 1/1 (frame 104/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 37.1ms\n",
      "video 1/1 (frame 105/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.7ms\n",
      "video 1/1 (frame 106/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.3ms\n",
      "video 1/1 (frame 107/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.4ms\n",
      "video 1/1 (frame 108/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 36.8ms\n",
      "video 1/1 (frame 109/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 36.7ms\n",
      "video 1/1 (frame 110/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 37.8ms\n",
      "video 1/1 (frame 111/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.9ms\n",
      "video 1/1 (frame 112/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 clock, 36.9ms\n",
      "video 1/1 (frame 113/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 clock, 36.1ms\n",
      "video 1/1 (frame 114/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.7ms\n",
      "video 1/1 (frame 115/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 clock, 36.3ms\n",
      "video 1/1 (frame 116/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 5 persons, 1 chair, 1 clock, 36.0ms\n",
      "video 1/1 (frame 117/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 36.7ms\n",
      "video 1/1 (frame 118/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 36.3ms\n",
      "video 1/1 (frame 119/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.4ms\n",
      "video 1/1 (frame 120/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.8ms\n",
      "video 1/1 (frame 121/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.4ms\n",
      "video 1/1 (frame 122/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 36.6ms\n",
      "video 1/1 (frame 123/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.1ms\n",
      "video 1/1 (frame 124/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.5ms\n",
      "video 1/1 (frame 125/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 2 clocks, 36.5ms\n",
      "video 1/1 (frame 126/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 36.7ms\n",
      "video 1/1 (frame 127/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.6ms\n",
      "video 1/1 (frame 128/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 train, 1 clock, 36.1ms\n",
      "video 1/1 (frame 129/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 4 persons, 1 chair, 1 clock, 36.4ms\n",
      "video 1/1 (frame 130/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 clock, 36.4ms\n",
      "video 1/1 (frame 131/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 persons, 1 chair, 1 clock, 37.0ms\n",
      "video 1/1 (frame 132/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 3 persons, 1 chair, 1 clock, 36.4ms\n",
      "video 1/1 (frame 133/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.8ms\n",
      "video 1/1 (frame 134/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 37.1ms\n",
      "video 1/1 (frame 135/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.5ms\n",
      "video 1/1 (frame 136/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.2ms\n",
      "video 1/1 (frame 137/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 35.5ms\n",
      "video 1/1 (frame 138/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.3ms\n",
      "video 1/1 (frame 139/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 35.8ms\n",
      "video 1/1 (frame 140/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.1ms\n",
      "video 1/1 (frame 141/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.7ms\n",
      "video 1/1 (frame 142/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 clocks, 35.9ms\n",
      "video 1/1 (frame 143/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.6ms\n",
      "video 1/1 (frame 144/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.3ms\n",
      "video 1/1 (frame 145/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.3ms\n",
      "video 1/1 (frame 146/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.6ms\n",
      "video 1/1 (frame 147/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.5ms\n",
      "video 1/1 (frame 148/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 2 clocks, 36.5ms\n",
      "video 1/1 (frame 149/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.0ms\n",
      "video 1/1 (frame 150/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.5ms\n",
      "video 1/1 (frame 151/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.1ms\n",
      "video 1/1 (frame 152/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.0ms\n",
      "video 1/1 (frame 153/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.4ms\n",
      "video 1/1 (frame 154/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 clock, 36.8ms\n",
      "video 1/1 (frame 155/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.6ms\n",
      "video 1/1 (frame 156/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.7ms\n",
      "video 1/1 (frame 157/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 person, 1 chair, 1 clock, 36.9ms\n",
      "video 1/1 (frame 158/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.8ms\n",
      "video 1/1 (frame 159/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 36.7ms\n",
      "video 1/1 (frame 160/160) /home/kazewong/Code/jupyter/data/videos/test.mp4: 1088x1920 1 clock, 37.0ms\n",
      "Speed: 5.2ms preprocess, 53.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1088, 1920)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"./data/models/yolo11x.pt\")\n",
    "model.export(format='openvino',imgsz=(1080,1920))\n",
    "model = YOLO(\"./data/models/yolo11x_openvino_model/\")\n",
    "results = model.track(source=\"data/videos/test.mp4\", show=True,imgsz=(1080,1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING ‚ö†Ô∏è imgsz=[1080, 1920] must be multiple of max stride 32, updating to [1088, 1920]\n",
      "0: 1088x1920 (no detections), 26.3ms\n",
      "Speed: 5.0ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 1088, 1920)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Get the boxes and track IDs\u001b[39;00m\n\u001b[32m     22\u001b[39m boxes = results[\u001b[32m0\u001b[39m].boxes.xywh.cpu()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m track_ids = \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint\u001b[49m().cpu().tolist()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Visualize the results on the frame\u001b[39;00m\n\u001b[32m     26\u001b[39m annotated_frame = results[\u001b[32m0\u001b[39m].plot()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = \"data/videos/test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Plot the tracks\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))  # x, y center point\n",
    "            if len(track) > 30:  # retain 30 tracks for 30 frames\n",
    "                track.pop(0)\n",
    "\n",
    "            # Draw the tracking lines\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
